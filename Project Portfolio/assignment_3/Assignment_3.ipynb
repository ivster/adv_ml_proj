{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ZcbEdkXFn0J"
   },
   "source": [
    "Ivy Wong\n",
    "\n",
    "xw2860\n",
    "\n",
    "Repo: https://github.com/ivster/adv_ml_proj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FLqbTUNEp_mw"
   },
   "source": [
    "Your final report should be written up in a Jupyter notebook.  It should be posted to a public Github repo as an ipynb  submitted to this assignment via courseworks.  Please include the link to your Github repo in this ipynb file.\n",
    "\n",
    "Use the deep learning and sklearn example ipynb notebooks from the Week 11 folder for example submission code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PLTIaMB3ChSW",
    "outputId": "cc529e8c-5449-46bb-a4e9-78cffa2d9eec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: aimodelshare==0.0.189 in /usr/local/lib/python3.9/dist-packages (0.0.189)\n",
      "Requirement already satisfied: shortuuid>=1.0.8 in /usr/local/lib/python3.9/dist-packages (from aimodelshare==0.0.189) (1.0.11)\n",
      "Requirement already satisfied: onnxmltools>=1.6.1 in /usr/local/lib/python3.9/dist-packages (from aimodelshare==0.0.189) (1.11.2)\n",
      "Requirement already satisfied: tf2onnx in /usr/local/lib/python3.9/dist-packages (from aimodelshare==0.0.189) (1.14.0)\n",
      "Requirement already satisfied: botocore==1.29.82 in /usr/local/lib/python3.9/dist-packages (from aimodelshare==0.0.189) (1.29.82)\n",
      "Requirement already satisfied: onnxruntime>=1.7.0 in /usr/local/lib/python3.9/dist-packages (from aimodelshare==0.0.189) (1.14.1)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.9/dist-packages (from aimodelshare==0.0.189) (2022.10.31)\n",
      "Requirement already satisfied: onnxconverter-common>=1.7.0 in /usr/local/lib/python3.9/dist-packages (from aimodelshare==0.0.189) (1.13.0)\n",
      "Requirement already satisfied: seaborn>=0.11.2 in /usr/local/lib/python3.9/dist-packages (from aimodelshare==0.0.189) (0.12.2)\n",
      "Requirement already satisfied: onnx==1.12.0 in /usr/local/lib/python3.9/dist-packages (from aimodelshare==0.0.189) (1.12.0)\n",
      "Requirement already satisfied: wget==3.2 in /usr/local/lib/python3.9/dist-packages (from aimodelshare==0.0.189) (3.2)\n",
      "Requirement already satisfied: protobuf==3.19.6 in /usr/local/lib/python3.9/dist-packages (from aimodelshare==0.0.189) (3.19.6)\n",
      "Requirement already satisfied: docker==5.0.0 in /usr/local/lib/python3.9/dist-packages (from aimodelshare==0.0.189) (5.0.0)\n",
      "Requirement already satisfied: psutil>=5.9.1 in /usr/local/lib/python3.9/dist-packages (from aimodelshare==0.0.189) (5.9.4)\n",
      "Requirement already satisfied: keras2onnx>=1.7.0 in /usr/local/lib/python3.9/dist-packages (from aimodelshare==0.0.189) (1.7.0)\n",
      "Requirement already satisfied: skl2onnx>=1.14.0 in /usr/local/lib/python3.9/dist-packages (from aimodelshare==0.0.189) (1.14.0)\n",
      "Requirement already satisfied: pathlib>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from aimodelshare==0.0.189) (1.0.1)\n",
      "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.9/dist-packages (from aimodelshare==0.0.189) (2.0.0+cu118)\n",
      "Requirement already satisfied: tensorflow==2.9.2 in /usr/local/lib/python3.9/dist-packages (from aimodelshare==0.0.189) (2.9.2)\n",
      "Requirement already satisfied: pydot==1.3.0 in /usr/local/lib/python3.9/dist-packages (from aimodelshare==0.0.189) (1.3.0)\n",
      "Requirement already satisfied: PyJWT>=2.4.0 in /usr/local/lib/python3.9/dist-packages (from aimodelshare==0.0.189) (2.6.0)\n",
      "Requirement already satisfied: Pympler==0.9 in /usr/local/lib/python3.9/dist-packages (from aimodelshare==0.0.189) (0.9)\n",
      "Requirement already satisfied: importlib-resources==5.10.0 in /usr/local/lib/python3.9/dist-packages (from aimodelshare==0.0.189) (5.10.0)\n",
      "Requirement already satisfied: scipy==1.7.0 in /usr/local/lib/python3.9/dist-packages (from aimodelshare==0.0.189) (1.7.0)\n",
      "Requirement already satisfied: boto3==1.26.69 in /usr/local/lib/python3.9/dist-packages (from aimodelshare==0.0.189) (1.26.69)\n",
      "Requirement already satisfied: scikit-learn==1.2.1 in /usr/local/lib/python3.9/dist-packages (from aimodelshare==0.0.189) (1.2.1)\n",
      "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.9/dist-packages (from aimodelshare==0.0.189) (1.6.3)\n",
      "Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.9/dist-packages (from astunparse==1.6.3->aimodelshare==0.0.189) (1.16.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse==1.6.3->aimodelshare==0.0.189) (0.40.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.9/dist-packages (from boto3==1.26.69->aimodelshare==0.0.189) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from boto3==1.26.69->aimodelshare==0.0.189) (0.6.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.9/dist-packages (from botocore==1.29.82->aimodelshare==0.0.189) (1.26.15)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.9/dist-packages (from botocore==1.29.82->aimodelshare==0.0.189) (2.8.2)\n",
      "Requirement already satisfied: requests!=2.18.0,>=2.14.2 in /usr/local/lib/python3.9/dist-packages (from docker==5.0.0->aimodelshare==0.0.189) (2.27.1)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /usr/local/lib/python3.9/dist-packages (from docker==5.0.0->aimodelshare==0.0.189) (1.5.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources==5.10.0->aimodelshare==0.0.189) (3.15.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.9/dist-packages (from onnx==1.12.0->aimodelshare==0.0.189) (1.22.4)\n",
      "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.9/dist-packages (from onnx==1.12.0->aimodelshare==0.0.189) (4.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.9/dist-packages (from pydot==1.3.0->aimodelshare==0.0.189) (3.0.9)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn==1.2.1->aimodelshare==0.0.189) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn==1.2.1->aimodelshare==0.0.189) (3.1.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.2->aimodelshare==0.0.189) (16.0.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.2->aimodelshare==0.0.189) (1.14.1)\n",
      "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.2->aimodelshare==0.0.189) (1.12)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.2->aimodelshare==0.0.189) (1.53.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.2->aimodelshare==0.0.189) (67.6.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.2->aimodelshare==0.0.189) (1.1.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.2->aimodelshare==0.0.189) (3.3.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.2->aimodelshare==0.0.189) (2.9.0)\n",
      "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.2->aimodelshare==0.0.189) (2.9.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.2->aimodelshare==0.0.189) (0.4.0)\n",
      "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.2->aimodelshare==0.0.189) (2.9.1)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.2->aimodelshare==0.0.189) (3.8.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.2->aimodelshare==0.0.189) (1.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.2->aimodelshare==0.0.189) (0.2.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.2->aimodelshare==0.0.189) (2.2.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.2->aimodelshare==0.0.189) (23.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.2->aimodelshare==0.0.189) (0.32.0)\n",
      "Requirement already satisfied: fire in /usr/local/lib/python3.9/dist-packages (from keras2onnx>=1.7.0->aimodelshare==0.0.189) (0.5.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from onnxruntime>=1.7.0->aimodelshare==0.0.189) (1.11.1)\n",
      "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.9/dist-packages (from onnxruntime>=1.7.0->aimodelshare==0.0.189) (15.0.1)\n",
      "Requirement already satisfied: pandas>=0.25 in /usr/local/lib/python3.9/dist-packages (from seaborn>=0.11.2->aimodelshare==0.0.189) (1.5.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in /usr/local/lib/python3.9/dist-packages (from seaborn>=0.11.2->aimodelshare==0.0.189) (3.7.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->aimodelshare==0.0.189) (3.11.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->aimodelshare==0.0.189) (3.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->aimodelshare==0.0.189) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->aimodelshare==0.0.189) (2.0.0)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.8.1->aimodelshare==0.0.189) (16.0.1)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.8.1->aimodelshare==0.0.189) (3.25.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn>=0.11.2->aimodelshare==0.0.189) (0.11.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn>=0.11.2->aimodelshare==0.0.189) (1.0.7)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn>=0.11.2->aimodelshare==0.0.189) (4.39.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn>=0.11.2->aimodelshare==0.0.189) (8.4.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn>=0.11.2->aimodelshare==0.0.189) (1.4.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=0.25->seaborn>=0.11.2->aimodelshare==0.0.189) (2022.7.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests!=2.18.0,>=2.14.2->docker==5.0.0->aimodelshare==0.0.189) (3.4)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests!=2.18.0,>=2.14.2->docker==5.0.0->aimodelshare==0.0.189) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests!=2.18.0,>=2.14.2->docker==5.0.0->aimodelshare==0.0.189) (2022.12.7)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.2->aimodelshare==0.0.189) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.2->aimodelshare==0.0.189) (1.8.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.2->aimodelshare==0.0.189) (2.17.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.2->aimodelshare==0.0.189) (2.2.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.2->aimodelshare==0.0.189) (0.6.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.2->aimodelshare==0.0.189) (3.4.3)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.9/dist-packages (from coloredlogs->onnxruntime>=1.7.0->aimodelshare==0.0.189) (10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.8.1->aimodelshare==0.0.189) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->onnxruntime>=1.7.0->aimodelshare==0.0.189) (1.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.2->aimodelshare==0.0.189) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.2->aimodelshare==0.0.189) (5.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.2->aimodelshare==0.0.189) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.2->aimodelshare==0.0.189) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow==2.9.2->aimodelshare==0.0.189) (6.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.2->aimodelshare==0.0.189) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.2->aimodelshare==0.0.189) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "#install aimodelshare library\n",
    "! pip install aimodelshare==0.0.189"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d3PiJXBhC5y-",
    "outputId": "ad4df9be-24ca-4a97-ffdb-001e5a0a0cce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading [=============================================>   ]\n",
      "\n",
      "Data downloaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Get competition data\n",
    "from aimodelshare import download_data\n",
    "download_data('public.ecr.aws/y2e2a1d6/sst2_competition_data-repository:latest') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jT0qFCZFNzHq",
    "outputId": "912b5e5d-ef02-4d85-dad2-96c319a41a67"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    The Rock is destined to be the 21st Century 's...\n",
       "1    The gorgeously elaborate continuation of `` Th...\n",
       "2    Singer/composer Bryan Adams contributes a slew...\n",
       "3                 Yet the act is still charming here .\n",
       "4    Whether or not you 're enlightened by any of D...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up X_train, X_test, and y_train_labels objects\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "\n",
    "X_train=pd.read_csv(\"sst2_competition_data/X_train.csv\", squeeze=True)\n",
    "X_test=pd.read_csv(\"sst2_competition_data/X_test.csv\", squeeze=True)\n",
    "\n",
    "y_train_labels=pd.read_csv(\"sst2_competition_data/y_train_labels.csv\", squeeze=True)\n",
    "\n",
    "# ohe encode Y data\n",
    "y_train = pd.get_dummies(y_train_labels)\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "16QV9Y9TC3B3",
    "outputId": "861e2a9d-927a-417a-ba7e-98980182fd91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6920, 40)\n",
      "(1821, 40)\n"
     ]
    }
   ],
   "source": [
    "# This preprocessor function makes use of the tf.keras tokenizer\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# Build vocabulary from training text data\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# preprocessor tokenizes words and makes sure all documents have the same length\n",
    "def preprocessor(data, maxlen=40, max_words=10000):\n",
    "\n",
    "    sequences = tokenizer.texts_to_sequences(data)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    X = pad_sequences(sequences, maxlen=maxlen)\n",
    "\n",
    "    return X\n",
    "\n",
    "print(preprocessor(X_train).shape)\n",
    "print(preprocessor(X_test).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "OooV0A5qDeMT",
    "outputId": "7853a963-591f-4b43-8b6b-b729b6fb46c1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-df483b5f-b396-4d9f-9e34-58355f669d94\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Negative</th>\n",
       "      <th>Positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6915</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6916</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6917</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6918</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6919</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6920 rows × 2 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-df483b5f-b396-4d9f-9e34-58355f669d94')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-df483b5f-b396-4d9f-9e34-58355f669d94 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-df483b5f-b396-4d9f-9e34-58355f669d94');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "      Negative  Positive\n",
       "0            0         1\n",
       "1            0         1\n",
       "2            0         1\n",
       "3            0         1\n",
       "4            0         1\n",
       "...        ...       ...\n",
       "6915         1         0\n",
       "6916         1         0\n",
       "6917         0         1\n",
       "6918         1         0\n",
       "6919         1         0\n",
       "\n",
       "[6920 rows x 2 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4VGacc0LDaMA",
    "outputId": "ebb6d867-f008-4084-e5d1-2ac45b8e4c85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your preprocessor is now saved to 'preprocessor.zip'\n"
     ]
    }
   ],
   "source": [
    "import aimodelshare as ai\n",
    "ai.export_preprocessor(preprocessor,\"\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RtgkM02MDpkO",
    "outputId": "44764e64-dca8-4e94-9cc7-8c258801a9a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Modelshare Username:··········\n",
      "AI Modelshare Password:··········\n",
      "AI Model Share login credentials set successfully.\n"
     ]
    }
   ],
   "source": [
    "#Set credentials using modelshare.org username/password\n",
    "\n",
    "from aimodelshare.aws import set_credentials\n",
    "    \n",
    "apiurl=\"https://rlxjxnoql9.execute-api.us-east-1.amazonaws.com/prod/m\" #This is the unique rest api that powers this specific Playground\n",
    "\n",
    "set_credentials(apiurl=apiurl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "fKNGSww8EGgi"
   },
   "outputs": [],
   "source": [
    "#Instantiate Competition\n",
    "\n",
    "mycompetition= ai.Competition(apiurl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zSDYY_FXxRne"
   },
   "source": [
    "## Discuss the dataset in general terms and describe why building a predictive model using this data might be practically useful.  Who could benefit from a model like this? Explain.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-o32XTri6ANz"
   },
   "source": [
    "#### The dataset used in this assignment is the Stanford Sentiment Treebank (SST), a dataset that contains movie reviews and is used for sentiment analysis. The reviews are broken down into two categories, either having positive or negative sentiment. Building a predictive model with this data is useful particularly for people from this specific industry, like media/ film businesses, because the model allows for them to analyze reviews and comments and gain an understanding of customer sentiment. In addition, understanding the type of sentiment customers are using also helps businesses in their product development stages. By analyzing the type of sentiment displayed by their cusomters, they can allocate resources necessarily. For example, using movies with positive sentiment as a baseline for the types of movies being produced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EhaAA6NaxT-6"
   },
   "source": [
    "## Run at least three prediction models to try to predict the SST sentiment dataset well.\n",
    "* Use an Embedding layer and LSTM layers in at least one model\n",
    "\n",
    "* Use an Embedding layer and Conv1d layers in at least one model\n",
    "\n",
    "* Use transfer learning with glove embeddings for at least one of these models\n",
    "\n",
    "* Submit your best three models to the leader board for the SST Model Share competition.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v5SfJk_ZryRA"
   },
   "source": [
    "### Embedding + LSTM Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6TEnIHMjsSIv",
    "outputId": "b2b0baa8-b2a8-4839-c2b1-905177c6c78e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "87/87 [==============================] - 14s 102ms/step - loss: 0.6671 - acc: 0.6145 - val_loss: 1.1402 - val_acc: 0.1488\n",
      "Epoch 2/3\n",
      "87/87 [==============================] - 6s 68ms/step - loss: 0.5625 - acc: 0.7070 - val_loss: 0.7326 - val_acc: 0.5838\n",
      "Epoch 3/3\n",
      "87/87 [==============================] - 7s 75ms/step - loss: 0.4222 - acc: 0.8083 - val_loss: 0.8772 - val_acc: 0.5405\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Flatten, GlobalMaxPooling2D\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, 16, input_length=40))\n",
    "model.add(LSTM(64, return_sequences=True, dropout=0.2))\n",
    "model.add(LSTM(32, return_sequences=True, dropout=0.2))\n",
    "model.add(LSTM(32, dropout=0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(preprocessor(X_train), y_train,\n",
    "                    epochs=3,\n",
    "                    batch_size=64,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pEhvnRiQDlY5"
   },
   "outputs": [],
   "source": [
    "# Save keras model to local ONNX file\n",
    "from aimodelshare.aimsonnx import model_to_onnx\n",
    "\n",
    "onnx_model = model_to_onnx(model, framework='keras',\n",
    "                          transfer_learning=False,\n",
    "                          deep_learning=True)\n",
    "with open(\"model.onnx\", \"wb\") as f:\n",
    "    f.write(onnx_model.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_Ql4wksyEUnP",
    "outputId": "f639e545-d688-482e-bb90-da97011d0329"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 3s 32ms/step\n",
      "Insert search tags to help users find your model (optional): \n",
      "Provide any useful notes about your model (optional): \n",
      "\n",
      "Your model has been submitted as model version 186\n",
      "\n",
      "To submit code used to create this model or to view current leaderboard navigate to Model Playground: \n",
      "\n",
      " https://www.modelshare.org/detail/model:2763\n"
     ]
    }
   ],
   "source": [
    "#Submit Model 1: \n",
    "\n",
    "#-- Generate predicted y values (Model 1)\n",
    "prediction_column_index=model.predict(preprocessor(X_test)).argmax(axis=1)\n",
    "\n",
    "# extract correct prediction labels \n",
    "prediction_labels = [y_train.columns[i] for i in prediction_column_index]\n",
    "\n",
    "# Submit Model 1 to Competition Leaderboard\n",
    "mycompetition.submit_model(model_filepath = \"model.onnx\",\n",
    "                                 preprocessor_filepath=\"preprocessor.zip\",\n",
    "                                 prediction_submission=prediction_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PVnubQZhr4lF"
   },
   "source": [
    "### Embedding + Conv1D Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F0wUwshCtMwi",
    "outputId": "5d08e2de-f2c8-44f5-8ecb-4ba4dc2098be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "173/173 [==============================] - 4s 14ms/step - loss: 0.6694 - acc: 0.6046 - val_loss: 0.8872 - val_acc: 0.1488\n",
      "Epoch 2/3\n",
      "173/173 [==============================] - 2s 13ms/step - loss: 0.6666 - acc: 0.6149 - val_loss: 0.8820 - val_acc: 0.1488\n",
      "Epoch 3/3\n",
      "173/173 [==============================] - 2s 13ms/step - loss: 0.6666 - acc: 0.6149 - val_loss: 0.8821 - val_acc: 0.1488\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Embedding(input_dim=10000, output_dim=128, input_length=40))\n",
    "model2.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "model2.add(Conv1D(filters=32, kernel_size=3, activation='relu'))\n",
    "model2.add(Conv1D(filters=32, kernel_size=3, activation='relu'))\n",
    "model2.add(Conv1D(filters=16, kernel_size=3, activation='relu'))\n",
    "model2.add(GlobalMaxPooling1D())\n",
    "model2.add(Dense(units=16, activation='softmax'))\n",
    "model2.add(Dense(units=2, activation='softmax'))\n",
    "\n",
    "\n",
    "model2.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model2.fit(preprocessor(X_train), y_train,\n",
    "                    epochs=3,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HEjwZpik3XtG"
   },
   "outputs": [],
   "source": [
    "# Save keras model to local ONNX file\n",
    "from aimodelshare.aimsonnx import model_to_onnx\n",
    "\n",
    "onnx_model2 = model_to_onnx(model2, framework='keras',\n",
    "                          transfer_learning=False,\n",
    "                          deep_learning=True)\n",
    "\n",
    "with open(\"model2.onnx\", \"wb\") as f:\n",
    "    f.write(onnx_model2.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n7Lmgckt3XtH",
    "outputId": "192c4585-2a1c-4fa5-8267-1c711e30fa7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 0s 5ms/step\n",
      "Insert search tags to help users find your model (optional): \n",
      "Provide any useful notes about your model (optional): \n",
      "\n",
      "Your model has been submitted as model version 187\n",
      "\n",
      "To submit code used to create this model or to view current leaderboard navigate to Model Playground: \n",
      "\n",
      " https://www.modelshare.org/detail/model:2763\n"
     ]
    }
   ],
   "source": [
    "#Submit Model 2: \n",
    "\n",
    "#-- Generate predicted y values (Model 2)\n",
    "prediction_column_index2=model2.predict(preprocessor(X_test)).argmax(axis=1)\n",
    "\n",
    "# extract correct prediction labels \n",
    "prediction_labels2 = [y_train.columns[i] for i in prediction_column_index2]\n",
    "\n",
    "# Submit Model 1 to Competition Leaderboard\n",
    "mycompetition.submit_model(model_filepath = \"model2.onnx\",\n",
    "                                 preprocessor_filepath=\"preprocessor.zip\",\n",
    "                                 prediction_submission=prediction_labels2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LRwPsCgOr6ot"
   },
   "source": [
    "### Transfer Learning + Glove Embedding on Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "950gVSU0vzZk",
    "outputId": "9116c39b-6349-4712-b751-a41a4bb1a294"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-04-18 00:51:39--  http://nlp.stanford.edu/data/wordvecs/glove.6B.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://nlp.stanford.edu/data/wordvecs/glove.6B.zip [following]\n",
      "--2023-04-18 00:51:39--  https://nlp.stanford.edu/data/wordvecs/glove.6B.zip\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://downloads.cs.stanford.edu/nlp/data/wordvecs/glove.6B.zip [following]\n",
      "--2023-04-18 00:51:39--  https://downloads.cs.stanford.edu/nlp/data/wordvecs/glove.6B.zip\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 862182753 (822M) [application/zip]\n",
      "Saving to: ‘glove.6B.zip.1’\n",
      "\n",
      "glove.6B.zip.1      100%[===================>] 822.24M  5.03MB/s    in 2m 42s  \n",
      "\n",
      "2023-04-18 00:54:21 (5.09 MB/s) - ‘glove.6B.zip.1’ saved [862182753/862182753]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download Glove embedding matrix weights (Might take 10 mins or so!)\n",
    "! wget http://nlp.stanford.edu/data/wordvecs/glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xHDZ80sAyWFR",
    "outputId": "feb558db-d3f6-4e01-d247-19da3cde5d46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  glove.6B.zip\n",
      "replace glove.6B.100d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
      "  inflating: glove.6B.100d.txt       \n",
      "replace glove.6B.200d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
      "replace glove.6B.300d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
     ]
    }
   ],
   "source": [
    "! unzip glove.6B.zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sdPAVxLcw0zo",
    "outputId": "eb2a306d-ae76-4d81-e101-7a1b0f80ceef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400001 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# Extract embedding data for 100 feature embedding matrix\n",
    "import os\n",
    "glove_dir = os.getcwd()\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_qCmxOV5vrUl",
    "outputId": "b0cd9f9d-6de2-4e64-a3ae-6b09c540eb4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13835 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# Determine number of unique tokens, and then use number of tokens in the embedding matrix\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "qYAaB4Z9w55d"
   },
   "outputs": [],
   "source": [
    "# Build embedding matrix\n",
    "embedding_dim = 100 # change if you use txt files using larger number of features\n",
    "max_words = 13835\n",
    "maxlen = 40\n",
    "\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if i < max_words:\n",
    "        if embedding_vector is not None:\n",
    "            # Words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JUYyuTl9SXtf",
    "outputId": "82eb6bf4-7479-431b-d8ef-c5003f702460"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13835, 100)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q0l0bOZOSpCh",
    "outputId": "9609f1d9-078d-4256-baa7-b47c9bf97162"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13835"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BZxUOF4Y0jc9"
   },
   "outputs": [],
   "source": [
    "# Update Model 2 to change input_dim and weights according to tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Flatten, Dense\n",
    "import numpy as np\n",
    "\n",
    "model3 = Sequential()\n",
    "model3.add(Embedding(max_words, embedding_dim, input_length=40, weights = [np.array([embeddings_index[word] if word in embeddings_index else np.zeros(100) for word in tokenizer.word_index.keys()])]))\n",
    "model3.add(LSTM(64, return_sequences=True, dropout=0.2))\n",
    "model3.add(LSTM(32, dropout=0.2))\n",
    "model3.add(Flatten())\n",
    "model3.add(Dense(2, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "16bo-2-_MQJQ",
    "outputId": "db742055-c350-4319-f958-b3556af27b31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "173/173 [==============================] - 15s 68ms/step - loss: 0.5934 - accuracy: 0.6678 - val_loss: 0.8282 - val_accuracy: 0.4581\n",
      "Epoch 2/10\n",
      "173/173 [==============================] - 13s 73ms/step - loss: 0.5197 - accuracy: 0.7354 - val_loss: 0.6280 - val_accuracy: 0.7095\n",
      "Epoch 3/10\n",
      "173/173 [==============================] - 15s 86ms/step - loss: 0.4910 - accuracy: 0.7534 - val_loss: 0.5805 - val_accuracy: 0.7392\n",
      "Epoch 4/10\n",
      "173/173 [==============================] - 9s 52ms/step - loss: 0.4756 - accuracy: 0.7661 - val_loss: 0.4891 - val_accuracy: 0.7861\n",
      "Epoch 5/10\n",
      "173/173 [==============================] - 13s 76ms/step - loss: 0.4503 - accuracy: 0.7778 - val_loss: 0.6138 - val_accuracy: 0.6720\n",
      "Epoch 6/10\n",
      "173/173 [==============================] - 8s 45ms/step - loss: 0.4353 - accuracy: 0.7903 - val_loss: 0.4000 - val_accuracy: 0.8497\n",
      "Epoch 7/10\n",
      "173/173 [==============================] - 12s 67ms/step - loss: 0.4182 - accuracy: 0.8035 - val_loss: 0.5892 - val_accuracy: 0.7211\n",
      "Epoch 8/10\n",
      "173/173 [==============================] - 8s 45ms/step - loss: 0.3898 - accuracy: 0.8150 - val_loss: 0.5315 - val_accuracy: 0.7608\n",
      "Epoch 9/10\n",
      "173/173 [==============================] - 11s 66ms/step - loss: 0.3765 - accuracy: 0.8206 - val_loss: 0.4172 - val_accuracy: 0.8237\n",
      "Epoch 10/10\n",
      "173/173 [==============================] - 8s 46ms/step - loss: 0.3506 - accuracy: 0.8405 - val_loss: 0.4140 - val_accuracy: 0.8273\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb4186abc10>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add weights as transfer learning \n",
    "\n",
    "model3.layers[0].set_weights([embedding_matrix])\n",
    "model3.layers[0].trainable = False\n",
    "\n",
    "# Compile the model\n",
    "model3.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model3.fit(preprocessor(X_train), y_train, validation_split = 0.2, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "W_piTYKZTxxT"
   },
   "outputs": [],
   "source": [
    "# Save keras model to local ONNX file\n",
    "from aimodelshare.aimsonnx import model_to_onnx\n",
    "\n",
    "onnx_model3 = model_to_onnx(model3, framework='keras',\n",
    "                          transfer_learning=True,\n",
    "                          deep_learning=True)\n",
    "\n",
    "with open(\"model3.onnx\", \"wb\") as f:\n",
    "    f.write(onnx_model3.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JpumSNC-TxxT",
    "outputId": "875fdd2b-e8f6-45f2-be2f-215d46310a26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 2s 16ms/step\n",
      "Insert search tags to help users find your model (optional): \n",
      "Provide any useful notes about your model (optional): \n",
      "\n",
      "Your model has been submitted as model version 342\n",
      "\n",
      "To submit code used to create this model or to view current leaderboard navigate to Model Playground: \n",
      "\n",
      " https://www.modelshare.org/detail/model:2763\n"
     ]
    }
   ],
   "source": [
    "#Submit Model 3: \n",
    "\n",
    "#-- Generate predicted y values (Model 3)\n",
    "prediction_column_index3=model3.predict(preprocessor(X_test)).argmax(axis=1)\n",
    "\n",
    "# extract correct prediction labels \n",
    "prediction_labels3 = [y_train.columns[i] for i in prediction_column_index3]\n",
    "\n",
    "# Submit Model 1 to Competition Leaderboard\n",
    "mycompetition.submit_model(model_filepath = \"model3.onnx\",\n",
    "                                 preprocessor_filepath=\"preprocessor.zip\",\n",
    "                                 prediction_submission=prediction_labels3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3soQANInEJCQ"
   },
   "source": [
    "## Discuss which models performed better and point out relevant hyper-parameter values for successful models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rWeXsrEfU535"
   },
   "source": [
    "#### Based on the 3 models ran above, the best performing model of the 3 is Model 3, where glove embedding and transfer learning layers were added onto Model 1. In this model, there was 1 Embedding model (where the weights were updated with GloVe embeddings), 2 LSTM models (each with 64 and 32 neurons respectively), and outputs to 2 categories: positive and negative sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I5Y7AlLexqmN"
   },
   "source": [
    "## After you submit your first three models, describe your best model with your team via your team slack channel\n",
    "* Fit and submit up to three more models after learning from your team.\n",
    "\n",
    "* Discuss results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hctu2Jz9U_d9"
   },
   "source": [
    "### Transfer Learning + Glove Embedding (more layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uP3Ff5cV9rZy",
    "outputId": "a7eb5fdc-4dba-490a-a3ac-8688db50925b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "87/87 [==============================] - 26s 241ms/step - loss: 0.6685 - acc: 0.6149 - val_loss: 1.1369 - val_acc: 0.1488\n",
      "Epoch 2/5\n",
      "87/87 [==============================] - 20s 234ms/step - loss: 0.6467 - acc: 0.6219 - val_loss: 1.2667 - val_acc: 0.1553\n",
      "Epoch 3/5\n",
      "87/87 [==============================] - 20s 236ms/step - loss: 0.6249 - acc: 0.6485 - val_loss: 0.8016 - val_acc: 0.4162\n",
      "Epoch 4/5\n",
      "87/87 [==============================] - 21s 239ms/step - loss: 0.5988 - acc: 0.6873 - val_loss: 1.1356 - val_acc: 0.2059\n",
      "Epoch 5/5\n",
      "87/87 [==============================] - 30s 342ms/step - loss: 0.5521 - acc: 0.7122 - val_loss: 0.6363 - val_acc: 0.6604\n"
     ]
    }
   ],
   "source": [
    "# Update Model 3 with more layers and neurons\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Flatten, Dense\n",
    "import numpy as np\n",
    "\n",
    "model4 = Sequential()\n",
    "model4.add(Embedding(max_words, embedding_dim, input_length=40, weights = [np.array([embeddings_index[word] if word in embeddings_index else np.zeros(100) for word in tokenizer.word_index.keys()])]))\n",
    "model4.add(LSTM(128, return_sequences=True, dropout=0.5))\n",
    "model4.add(LSTM(64, dropout=0.5))\n",
    "model4.add(Flatten())\n",
    "model4.add(Dense(2, activation='softmax'))\n",
    "\n",
    "\n",
    "model4.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model4.fit(preprocessor(X_train), y_train,\n",
    "                    epochs=5,\n",
    "                    batch_size=64,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "mikzZUST_Z_S"
   },
   "outputs": [],
   "source": [
    "# Save keras model to local ONNX file\n",
    "from aimodelshare.aimsonnx import model_to_onnx\n",
    "\n",
    "onnx_model4 = model_to_onnx(model4, framework='keras',\n",
    "                          transfer_learning=True,\n",
    "                          deep_learning=True)\n",
    "\n",
    "with open(\"model4.onnx\", \"wb\") as f:\n",
    "    f.write(onnx_model4.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "943ppVl3_Z_T",
    "outputId": "6ef62b8f-77bf-41e0-d36a-2706e92a5372"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 3s 40ms/step\n",
      "Insert search tags to help users find your model (optional): \n",
      "Provide any useful notes about your model (optional): \n",
      "\n",
      "Your model has been submitted as model version 351\n",
      "\n",
      "To submit code used to create this model or to view current leaderboard navigate to Model Playground: \n",
      "\n",
      " https://www.modelshare.org/detail/model:2763\n"
     ]
    }
   ],
   "source": [
    "#Submit Model 4: \n",
    "\n",
    "#-- Generate predicted y values (Model 4)\n",
    "prediction_column_index4=model4.predict(preprocessor(X_test)).argmax(axis=1)\n",
    "\n",
    "# extract correct prediction labels \n",
    "prediction_labels4 = [y_train.columns[i] for i in prediction_column_index4]\n",
    "\n",
    "# Submit Model 1 to Competition Leaderboard\n",
    "mycompetition.submit_model(model_filepath = \"model4.onnx\",\n",
    "                                 preprocessor_filepath=\"preprocessor.zip\",\n",
    "                                 prediction_submission=prediction_labels4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dz613XtHVHYK"
   },
   "source": [
    "### Embedding + LSTM (with more layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ybb4NYgDnYl1"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Flatten, GlobalMaxPooling2D\n",
    "\n",
    "model5 = tf.keras.Sequential()\n",
    "model5.add(keras.layers.Embedding(10000, 16, input_length=40))\n",
    "model5.add(LSTM(256, return_sequences=True, dropout=0.2)) # Add layer with more neurons\n",
    "model5.add(LSTM(128, return_sequences=True, dropout=0.2)) \n",
    "model5.add(LSTM(128, return_sequences=True, dropout=0.2))\n",
    "model5.add(LSTM(64, return_sequences=True, dropout=0.2))\n",
    "model5.add(LSTM(64, return_sequences=True, dropout=0.2))\n",
    "model5.add(LSTM(32, dropout=0.2))\n",
    "model5.add(Flatten())\n",
    "model5.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KQr1l6BcwLF4",
    "outputId": "10d539e9-c910-4f32-e7d6-ba85e027768b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "87/87 [==============================] - 68s 640ms/step - loss: 0.6705 - acc: 0.6149 - val_loss: 0.8726 - val_acc: 0.1488\n",
      "Epoch 2/3\n",
      "87/87 [==============================] - 53s 610ms/step - loss: 0.6700 - acc: 0.6120 - val_loss: 0.8283 - val_acc: 0.1488\n",
      "Epoch 3/3\n",
      "87/87 [==============================] - 54s 618ms/step - loss: 0.6703 - acc: 0.6089 - val_loss: 0.8468 - val_acc: 0.1488\n"
     ]
    }
   ],
   "source": [
    "model5.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model5.fit(preprocessor(X_train), y_train, epochs=3, batch_size=64, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y3Wb_psDXEsK"
   },
   "outputs": [],
   "source": [
    "# Save keras model to local ONNX file\n",
    "from aimodelshare.aimsonnx import model_to_onnx\n",
    "\n",
    "onnx_model5 = model_to_onnx(model5, framework='keras',\n",
    "                          transfer_learning=False,\n",
    "                          deep_learning=True)\n",
    "\n",
    "with open(\"model5.onnx\", \"wb\") as f:\n",
    "    f.write(onnx_model5.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pNmfNOXSXEsK",
    "outputId": "25fdb04e-ec45-4d92-886c-c101d65b92cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 10s 113ms/step\n",
      "Insert search tags to help users find your model (optional): \n",
      "Provide any useful notes about your model (optional): \n",
      "\n",
      "Your model has been submitted as model version 338\n",
      "\n",
      "To submit code used to create this model or to view current leaderboard navigate to Model Playground: \n",
      "\n",
      " https://www.modelshare.org/detail/model:2763\n"
     ]
    }
   ],
   "source": [
    "#Submit Model 5: \n",
    "\n",
    "#-- Generate predicted y values (Model 5)\n",
    "prediction_column_index5=model5.predict(preprocessor(X_test)).argmax(axis=1)\n",
    "\n",
    "# extract correct prediction labels \n",
    "prediction_labels5 = [y_train.columns[i] for i in prediction_column_index5]\n",
    "\n",
    "# Submit Model 1 to Competition Leaderboard\n",
    "mycompetition.submit_model(model_filepath = \"model5.onnx\",\n",
    "                                 preprocessor_filepath=\"preprocessor.zip\",\n",
    "                                 prediction_submission=prediction_labels5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-L0M09fJVLFC"
   },
   "source": [
    "### Embedding + Conv1D (with more layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ImCJp2gkylJL"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, Dense\n",
    "\n",
    "model6 = Sequential()\n",
    "model6.add(Embedding(input_dim=10000, output_dim=128, input_length=40))\n",
    "model6.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "model6.add(MaxPooling1D(pool_size=2))\n",
    "model6.add(Conv1D(filters=32, kernel_size=3, activation='relu'))\n",
    "model6.add(MaxPooling1D(pool_size=2))\n",
    "model6.add(Conv1D(filters=32, kernel_size=3, activation='relu'))\n",
    "model6.add(MaxPooling1D(pool_size=2))\n",
    "model6.add(Conv1D(filters=16, kernel_size=3, activation='relu'))\n",
    "model6.add(GlobalMaxPooling1D())\n",
    "model6.add(Dense(units=16, activation='softmax'))\n",
    "model6.add(Dense(units=2, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wUNFrzl8ySP-",
    "outputId": "a9be86f4-8835-46bf-99d5-c9c09b504357"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "173/173 [==============================] - 5s 20ms/step - loss: 0.6676 - acc: 0.6120 - val_loss: 0.8493 - val_acc: 0.1488\n",
      "Epoch 2/3\n",
      "173/173 [==============================] - 5s 30ms/step - loss: 0.6063 - acc: 0.6938 - val_loss: 0.6940 - val_acc: 0.6134\n",
      "Epoch 3/3\n",
      "173/173 [==============================] - 7s 43ms/step - loss: 0.5060 - acc: 0.8279 - val_loss: 0.6294 - val_acc: 0.7247\n"
     ]
    }
   ],
   "source": [
    "model6.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model6.fit(preprocessor(X_train), y_train,\n",
    "                    epochs=3,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S_RPECXHylJM"
   },
   "outputs": [],
   "source": [
    "# Save keras model to local ONNX file\n",
    "from aimodelshare.aimsonnx import model_to_onnx\n",
    "\n",
    "onnx_model6 = model_to_onnx(model6, framework='keras',\n",
    "                          transfer_learning=False,\n",
    "                          deep_learning=True)\n",
    "\n",
    "with open(\"model6.onnx\", \"wb\") as f:\n",
    "    f.write(onnx_model6.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qOioZNiGylJM",
    "outputId": "62097f25-af39-432c-87a5-ccbe85d01421"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 1s 6ms/step\n",
      "Insert search tags to help users find your model (optional): \n",
      "Provide any useful notes about your model (optional): \n",
      "\n",
      "Your model has been submitted as model version 340\n",
      "\n",
      "To submit code used to create this model or to view current leaderboard navigate to Model Playground: \n",
      "\n",
      " https://www.modelshare.org/detail/model:2763\n"
     ]
    }
   ],
   "source": [
    "#Submit Model 6: \n",
    "\n",
    "#-- Generate predicted y values (Model 6)\n",
    "prediction_column_index6=model6.predict(preprocessor(X_test)).argmax(axis=1)\n",
    "\n",
    "# extract correct prediction labels \n",
    "prediction_labels6 = [y_train.columns[i] for i in prediction_column_index6]\n",
    "\n",
    "# Submit Model 1 to Competition Leaderboard\n",
    "mycompetition.submit_model(model_filepath = \"model6.onnx\",\n",
    "                                 preprocessor_filepath=\"preprocessor.zip\",\n",
    "                                 prediction_submission=prediction_labels6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4-oLdDxH_fji"
   },
   "source": [
    "### Transfer Learning + Glove Embedding on Embedding and Dense layers (with higher neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "mVgQ_AsRPmRo"
   },
   "outputs": [],
   "source": [
    "# Create a new preprocessor by taking one-hot encoding of the labels after tokenizing and padding the text data. \n",
    "\n",
    "def preprocessor2(data, labels, maxlen=40, max_words=10000):\n",
    "    sequences = tokenizer.texts_to_sequences(data)\n",
    "    X = pad_sequences(sequences, maxlen=maxlen)\n",
    "    y = pd.get_dummies(labels)\n",
    "    return X, y\n",
    "\n",
    "X_train_processed, y_train_processed = preprocessor2(X_train, y_train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fQ7gkQcEVGvQ",
    "outputId": "9177ccd8-828f-47c0-c0cb-8e3a167bbd3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_19 (Embedding)    (None, 40, 100)           1383500   \n",
      "                                                                 \n",
      " flatten_12 (Flatten)        (None, 4000)              0         \n",
      "                                                                 \n",
      " dense_30 (Dense)            (None, 6412)              25654412  \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 6412)              0         \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, 3214)              20611382  \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 2)                 6430      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 47,655,724\n",
      "Trainable params: 46,272,224\n",
      "Non-trainable params: 1,383,500\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "173/173 [==============================] - 112s 639ms/step - loss: 0.6904 - accuracy: 0.6210 - val_loss: 0.8416 - val_accuracy: 0.4364\n",
      "Epoch 2/10\n",
      "173/173 [==============================] - 100s 579ms/step - loss: 0.5370 - accuracy: 0.7229 - val_loss: 0.8041 - val_accuracy: 0.5759\n",
      "Epoch 3/10\n",
      "173/173 [==============================] - 104s 599ms/step - loss: 0.4713 - accuracy: 0.7706 - val_loss: 0.8677 - val_accuracy: 0.5361\n",
      "Epoch 4/10\n",
      "173/173 [==============================] - 107s 621ms/step - loss: 0.4010 - accuracy: 0.8107 - val_loss: 0.6962 - val_accuracy: 0.7052\n",
      "Epoch 5/10\n",
      "173/173 [==============================] - 103s 592ms/step - loss: 0.3458 - accuracy: 0.8421 - val_loss: 1.1109 - val_accuracy: 0.5065\n",
      "Epoch 6/10\n",
      "173/173 [==============================] - 101s 583ms/step - loss: 0.2771 - accuracy: 0.8835 - val_loss: 0.9477 - val_accuracy: 0.6669\n",
      "Epoch 7/10\n",
      "173/173 [==============================] - 100s 581ms/step - loss: 0.2274 - accuracy: 0.9068 - val_loss: 1.0936 - val_accuracy: 0.5961\n",
      "Epoch 8/10\n",
      "173/173 [==============================] - 100s 580ms/step - loss: 0.1778 - accuracy: 0.9256 - val_loss: 1.0614 - val_accuracy: 0.6546\n",
      "Epoch 9/10\n",
      "173/173 [==============================] - 101s 583ms/step - loss: 0.1548 - accuracy: 0.9382 - val_loss: 1.3971 - val_accuracy: 0.5824\n",
      "Epoch 10/10\n",
      "173/173 [==============================] - 102s 591ms/step - loss: 0.1182 - accuracy: 0.9538 - val_loss: 1.8518 - val_accuracy: 0.6149\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8d82a39640>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model7 = tf.keras.Sequential()\n",
    "model7.add(keras.layers.Embedding(input_dim=len(tokenizer.word_index),\n",
    "                           output_dim=100,\n",
    "                           input_length=maxlen,\n",
    "                           trainable=False,\n",
    "                           weights=[np.array([embeddings_index[word] if word in embeddings_index else np.zeros(100) for word in tokenizer.word_index.keys()])]))\n",
    "model7.add(tf.keras.layers.Flatten())\n",
    "model7.add(tf.keras.layers.Dense(6412, activation='relu'))  # Added additional Dense layer with 64 neurons and ReLU activation\n",
    "model7.add(tf.keras.layers.Dropout(0.5))  # Added Dropout layer with dropout rate of 0.5\n",
    "model7.add(tf.keras.layers.Dense(3214, activation='relu'))  # Added additional Dense layer with 32 neurons and ReLU activation\n",
    "model7.add(tf.keras.layers.Dense(2, activation='softmax'))\n",
    "model7.summary()\n",
    "\n",
    "model7.layers[0].set_weights([embedding_matrix])\n",
    "model7.layers[0].trainable = False\n",
    "\n",
    "# Compile the model\n",
    "model7.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model7.fit(X_train_processed, y_train_processed, validation_split=0.2, epochs=10, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "M11_Pe8eWkZl"
   },
   "outputs": [],
   "source": [
    "# Save keras model to local ONNX file\n",
    "from aimodelshare.aimsonnx import model_to_onnx\n",
    "\n",
    "onnx_model7 = model_to_onnx(model7, framework='keras',\n",
    "                          transfer_learning=True,\n",
    "                          deep_learning=True)\n",
    "\n",
    "with open(\"model7.onnx\", \"wb\") as f:\n",
    "    f.write(onnx_model7.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X4sKPUhGWkZm",
    "outputId": "e8d66c2a-69e3-481f-b4f1-b4f3f4cdc614"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 5s 93ms/step\n",
      "Insert search tags to help users find your model (optional): \n",
      "Provide any useful notes about your model (optional): \n",
      "\n",
      "Your model has been submitted as model version 323\n",
      "\n",
      "To submit code used to create this model or to view current leaderboard navigate to Model Playground: \n",
      "\n",
      " https://www.modelshare.org/detail/model:2763\n"
     ]
    }
   ],
   "source": [
    "#Submit Model 7: \n",
    "\n",
    "#-- Generate predicted y values (Model 7)\n",
    "prediction_column_index7=model7.predict(preprocessor(X_test)).argmax(axis=1)\n",
    "\n",
    "# extract correct prediction labels \n",
    "prediction_labels7 = [y_train.columns[i] for i in prediction_column_index7]\n",
    "\n",
    "# Submit Model 1 to Competition Leaderboard\n",
    "mycompetition.submit_model(model_filepath = \"model7.onnx\",\n",
    "                                 preprocessor_filepath=\"preprocessor.zip\",\n",
    "                                 prediction_submission=prediction_labels7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lcz-dSQQxunG"
   },
   "source": [
    "## Discuss which models you tried and which models performed better and point out relevant hyper-parameter values for successful models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZRWexiBpzLWG"
   },
   "source": [
    "#### After submitting the new set of models again, this time the best performing model is the Model 6, the model with embedding and Conv1D layers. In this model, more layers and max pooling were added to improve the performance of the model. This model has 1 Embedding layer, 4 Conv1D layers, 3 Max Pooling layers, and 1 Global Max Pooling layer. This model outputs to 2 categories: positive or negative sentiment."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
